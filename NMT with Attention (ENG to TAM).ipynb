{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb81219-6dba-45ee-88f3-97eb55666a94",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67769c3b-fbd2-4654-a71c-7fa1c2069c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b806824-aaf4-4d0b-9bc9-fa47864bbc8d",
   "metadata": {},
   "source": [
    "## Preprocessing to feed into Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907cd65-51a1-4e05-a072-8f64bc68b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = 'F:\\Datasets\\data.en1'\n",
    "file_path2 = 'F:\\Datasets\\data.en2'\n",
    "file_path3 = 'F:\\Datasets\\data.en3'\n",
    "eng_lines1 = []\n",
    "eng_lines2 = []\n",
    "eng_lines3 = []\n",
    "\n",
    "with open(file_path1, 'r', encoding='utf-8') as file:\n",
    "    eng_lines1 = [line.strip() for line in file.readlines()]\n",
    "with open(file_path2, 'r', encoding='utf-8') as file:\n",
    "    eng_lines2 = [line.strip() for line in file.readlines()]\n",
    "with open(file_path3, 'r', encoding='utf-8') as file:\n",
    "    eng_lines3 = [line.strip() for line in file.readlines()]\n",
    "\n",
    "eng_lines = eng_lines1 + eng_lines2 + eng_lines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b9a11-87b9-4344-b1d0-9c3978efdb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path4 = 'F:\\Datasets\\data.ta1'\n",
    "file_path5 = 'F:\\Datasets\\data.ta2'\n",
    "file_path6 = 'F:\\Datasets\\data.ta3'\n",
    "tam_lines1 = []\n",
    "tam_lines2 = []\n",
    "tam_lines3 = []\n",
    "\n",
    "with open(file_path4, 'r', encoding='utf-8') as file:\n",
    "    tam_lines1 = [line.strip() for line in file.readlines()]\n",
    "with open(file_path5, 'r', encoding='utf-8') as file:\n",
    "    tam_lines2 = [line.strip() for line in file.readlines()]\n",
    "with open(file_path6, 'r', encoding='utf-8') as file:\n",
    "    tam_lines3 = [line.strip() for line in file.readlines()]\n",
    "\n",
    "tam_lines = tam_lines1 + tam_lines2 + tam_lines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c95255-d83f-49e9-bab7-9138e2fc0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = np.array(eng_lines)\n",
    "target = np.array(tam_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37d05e-9dca-4873-9383-745de91292c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context[is_train], target[is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context[~is_train], target[~is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff2415-af16-41fa-ad93-08318ff69276",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = tf.constant('இயற்கை')\n",
    "print(example_text.numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c669028a-3b12-47bc-a70d-2951e469463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿\\u0B80-\\u0BFF]', '')\n",
    "    text = tf.strings.regex_replace(text, r'([.?!,¿])', r' \\1 ')\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join(['[SOS]', text, '[EOS]'], separator=' ')\n",
    "    return text\n",
    "\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3b819-c94d-48de-926b-1b66cc3d46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a49059-9f55-4c62-93e6-57c85b560520",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)\n",
    "\n",
    "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
    "context_text_processor.get_vocabulary()[:10]\n",
    "context_text_processor.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e010fd5-610a-49d6-b2a1-db719da47ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)\n",
    "\n",
    "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
    "target_text_processor.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54bb76a-8044-45db-830a-95f0ac18d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "  context = context_text_processor(context).to_tensor()\n",
    "  target = target_text_processor(target)\n",
    "  targ_in = target[:,:-1].to_tensor()\n",
    "  targ_out = target[:,1:].to_tensor()\n",
    "  return (context, targ_in), targ_out\n",
    "    \n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd97d13-63a6-4013-8279-948f729e93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (to_translate, sr_translation), translation in train_ds.take(1):\n",
    "  print(to_translate[0, :].numpy()) \n",
    "  print()\n",
    "  print(sr_translation[0, :].numpy()) \n",
    "  print(translation[0, :].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129368f-510c-446f-b8ae-e79997e6a4d2",
   "metadata": {},
   "source": [
    "# Build the Encoder, Decoder and Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f2c9f-f196-474e-8326-05e3a88aef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e5a2e-e317-4bc5-a0b8-ad877aa2b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(  \n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )  \n",
    "\n",
    "        self.rnn = tf.keras.layers.Bidirectional(  \n",
    "            merge_mode=\"sum\",  \n",
    "            layer=tf.keras.layers.LSTM(\n",
    "                units=units,\n",
    "                return_sequences=True\n",
    "            ),  \n",
    "        )  \n",
    "\n",
    "    def call(self, context):\n",
    "        x = self.embedding(context)\n",
    "        x = self.rnn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a319ed-c88b-4e56-8443-f3e8339a8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "encoder_output = encoder(to_translate)\n",
    "\n",
    "print(f'Tensor of sentences in english has shape: {to_translate.shape}\\n')\n",
    "print(f'Encoder output has shape: {encoder_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6410b-3f18-438d-9913-0daaa819eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = ( \n",
    "            tf.keras.layers.MultiHeadAttention(\n",
    "                key_dim=units,\n",
    "                num_heads=1\n",
    "            ) \n",
    "        )  \n",
    "\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, context, target):\n",
    "        attn_output = self.mha(\n",
    "            query=target,\n",
    "            value=context\n",
    "        )\n",
    "\n",
    "        x = self.add([target, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e10027-3220-488c-86ce-3722859e604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "sr_translation_embed = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=UNITS, mask_zero=True)(sr_translation)\n",
    "\n",
    "attention_result = attention_layer(encoder_output, sr_translation_embed)\n",
    "\n",
    "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
    "print(f'Tensor of translations has shape: {sr_translation_embed.shape}')\n",
    "print(f'Tensor of attention scores has shape: {attention_result.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b691dd-ea50-45dd-8d4b-2aac642612f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )  \n",
    "\n",
    "        self.pre_attention_rnn = tf.keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )  \n",
    "\n",
    "        self.attention = CrossAttention(units)\n",
    "\n",
    "        self.post_attention_rnn = tf.keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True\n",
    "        )  \n",
    "\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=vocab_size,\n",
    "            activation=tf.nn.log_softmax\n",
    "        )  \n",
    "\n",
    "    def call(self, context, target, state=None, return_state=False):\n",
    "        \n",
    "        x = self.embedding(target)       \n",
    "        x, hidden_state, cell_state = self.pre_attention_rnn(x, initial_state=state)\n",
    "        x = self.attention(context, x)\n",
    "        x = self.post_attention_rnn(x)\n",
    "        logits = self.output_layer(x)\n",
    "        \n",
    "        if return_state:\n",
    "            return logits, [hidden_state, cell_state]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca99626-b98a-4b40-8096-b5281f3499dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "logits = decoder(encoder_output, sr_translation)\n",
    "\n",
    "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
    "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
    "print(f'Tensor of logits has shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d70510-4f03-4a38-a218-998973b99358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(vocab_size, units)\n",
    "        self.decoder = Decoder(vocab_size, units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context, target = inputs\n",
    "\n",
    "        encoded_context = self.encoder(context)\n",
    "        logits = self.decoder(encoded_context, target)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cbd5b3-9838-4079-99e2-197d558f25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(VOCAB_SIZE, UNITS)\n",
    "\n",
    "logits = translator((to_translate, sr_translation))\n",
    "\n",
    "print(f'Tensor of sentences to translate has shape: {to_translate.shape}')\n",
    "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
    "print(f'Tensor of logits has shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72115e1-86a3-491a-ad82-e5788389513c",
   "metadata": {},
   "source": [
    "# Compile and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51103b-f242-4fdc-bddf-671aaaad5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    \n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "def masked_acc(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341696a-ec3f-485e-90b9-ab5a6269a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train(model, epochs=100, steps_per_epoch=100):\n",
    "    model.compile(optimizer=\"adam\", loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds.repeat(),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_ds,\n",
    "        validation_steps=20,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be6290-7886-46f5-8fb2-79b623e9946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = compile_and_train(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fe1d0-1e61-4bb9-9505-06b63d8172c8",
   "metadata": {},
   "source": [
    "# Some Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3baa9-7e3a-4896-9e38-0f33feda097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = tf.keras.layers.StringLookup(\n",
    "    vocabulary=target_text_processor.get_vocabulary(), \n",
    "    mask_token=\"\", \n",
    "    oov_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "id_to_word = tf.keras.layers.StringLookup(\n",
    "    vocabulary=target_text_processor.get_vocabulary(),\n",
    "    mask_token=\"\",\n",
    "    oov_token=\"[UNK]\",\n",
    "    invert=True,\n",
    ")\n",
    "\n",
    "def tokens_to_text(tokens, id_to_word):\n",
    "    words = id_to_word(tokens)\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "unk_id = word_to_id(\"[UNK]\")\n",
    "sos_id = word_to_id(\"[SOS]\")\n",
    "eos_id = word_to_id(\"[EOS]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19c5c9-90ca-4f0e-a6d5-131df11e38e9",
   "metadata": {},
   "source": [
    "# Minimum Bayes Risk Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8338e0a-561c-4fd2-9c5f-07086ffcded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
    "    \n",
    "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
    "    \n",
    "    logits = logits[:, -1, :]\n",
    "        \n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)     \n",
    "    else:\n",
    "        logits = logits / temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "    \n",
    "    logits = tf.squeeze(logits)\n",
    "    next_token = tf.squeeze(next_token)\n",
    "    \n",
    "    logit = logits[next_token].numpy()\n",
    "    \n",
    "    next_token = tf.reshape(next_token, shape=(1,1))\n",
    "\n",
    "    if next_token == eos_id:\n",
    "        done = True\n",
    "    \n",
    "    return next_token, logit, state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2c1a6-c056-41f0-b2dc-08d45327fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, text, max_length=50, temperature=0.0):\n",
    "    tokens, logits = [], []\n",
    "    \n",
    "    text = tf.convert_to_tensor(text)[tf.newaxis]\n",
    "    context = english_vectorizer(text).to_tensor()\n",
    "    context = model.encoder(context)\n",
    "\n",
    "    next_token = tf.fill((1,1), sos_id)\n",
    "    \n",
    "    state = [tf.zeros((1, UNITS)), tf.zeros((1, UNITS))]\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        next_token, logit, state, done = generate_next_token(decoder=model.decoder, context=context, next_token=next_token, done=done, state=state,temperature=temperature)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "        tokens.append(next_token)\n",
    "        logits.append(logit)\n",
    "    \n",
    "    tokens = tf.concat(tokens, axis=-1)\n",
    "    \n",
    "    translation = tf.squeeze(tokens_to_text(tokens, id_to_word))\n",
    "    translation = translation.numpy().decode()\n",
    "    \n",
    "    return translation, logits[-1], tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2585a00-045b-43cf-bfe7-ee9a77af9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, text, n_samples=4, temperature=0.6):\n",
    "    \n",
    "    samples, log_probs = [], []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        _, logp, sample = translate(model, text, temperature=temperature)\n",
    "        samples.append(np.squeeze(sample.numpy()).tolist())\n",
    "        log_probs.append(logp)\n",
    "                \n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aaacdf-ef7b-4877-9695-85224780c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(candidate, reference):\n",
    "        \n",
    "    candidate_set = set(candidate)\n",
    "    reference_set = set(reference)\n",
    "    \n",
    "    common_tokens = candidate_set.intersection(reference_set)\n",
    "    \n",
    "    all_tokens = candidate_set.union(reference_set)\n",
    "    \n",
    "    overlap = len(common_tokens) / len(all_tokens)\n",
    "        \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e016a3-0c93-448d-a82e-61307a7276b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_overlap(samples, log_probs, similarity_fn):\n",
    "    scores = {}\n",
    "    \n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        overlap, weight_sum = 0.0, 0.0\n",
    "        \n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
    "           \n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            sample_p = float(np.exp(logp))\n",
    "            weight_sum += sample_p\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            overlap += sample_p * sample_overlap\n",
    "            \n",
    "        score = overlap / weight_sum\n",
    "        score = round(score, 3)\n",
    "        \n",
    "        scores[index_candidate] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74533146-3ed7-4cbe-a115-061fbafde459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=jaccard_similarity):\n",
    "\n",
    "    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n",
    "    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n",
    "    decoded_translations = [tokens_to_text(s, id_to_word).numpy().decode('utf-8') for s in samples]\n",
    "    max_score_key = max(scores, key=lambda k: scores[k])\n",
    "    translation = decoded_translations[max_score_key]\n",
    "    \n",
    "    return translation, decoded_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6d1bd-9e36-4590-835e-a1faa79d61df",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e00b3-f509-4133-8009-ae2634dc7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentence = \"I love languages\"\n",
    "\n",
    "translation, candidates = mbr_decode(model, english_sentence, n_samples=10, temperature=0.6)\n",
    "\n",
    "print(\"Translation candidates:\")\n",
    "for c in candidates:\n",
    "    print(c)\n",
    "\n",
    "print(f\"\\nSelected translation: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlkernel",
   "language": "python",
   "name": "mlkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
